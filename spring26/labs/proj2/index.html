<!DOCTYPE html><html lang="en-us"><head><meta charset="utf-8"><title>Home</title><base href="/spring26/labs/lab10/"><meta content="width=device-width, initial-scale=1" name="viewport"><style>@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/spring26/assets/font/tex-gyre-heros/texgyreheros-regular.otf") format("opentype");
    font-weight: regular;
    font-style: regular;
}
@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/spring26/assets/font/tex-gyre-heros/texgyreheros-bold.otf") format("opentype");
    font-weight: bold;
    font-style: regular;
}
@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/spring26/assets/font/tex-gyre-heros/texgyreheros-italic.otf") format("opentype");
    font-weight: regular;
    font-style: italic;
}
@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/spring26/assets/font/tex-gyre-heros/texgyreheros-bolditalic.otf") format("opentype");
    font-weight: bold;
    font-style: italic;
}</style><link href="/spring26/assets/main.css" rel="stylesheet">
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" rel="stylesheet">
<link href="/spring26/assets/favicon.png" rel="icon" type="image/png">
</head>

<body><header><nav>
<h1><a href="/spring26/">Home</a></h1>
<p><a href="/spring26/calendar">Calendar</a></p>
<p><a href="/spring26/labs">Labs</a></p>
<p><a href="/spring26/lectures">Lectures</a></p>
<p><a href="/spring26/syllabus">Syllabus</a></p>
<p><a href="/spring26/resources">Resources</a></p>
<p><a href="/spring26/contact">Contact</a></p>
<p><a href="/spring26/piazza">Piazza</a></p>
</nav></header>

<main>
<h1>Project 2: Distributed Programming with MPI</h1>

<h2>Prologue: Logistics</h2>
<h3>Due Dates</h3>
<p>For this lab, you’ll be turning in the following deliverables:</p>
<ul>
<li>
<p><strong>Checkpoint</strong>: Due Friday, February 14, 11:59 pm. 
    Submit your completed code in XXX, your responses to Questions XXX, and a discussion of your progress on Part 1.</p>
</li>
<li>
<p><strong>Final Submission</strong>: Due Friday, November 21, 11:59 pm. 
    Submit your completed code for all parts of the lab, 
    as well as a PDF writeup containing your answers for Question XXX of the final writeup.</p>
</li>
</ul>
<h3>Starter Code</h3>
<p>You can get the starter code for this lab by cloning the <a href="https://github.com/accelerated-computing-class/lab12">lab
repository</a>:</p>
<pre><span class="highlight-source highlight-shell highlight-bash"><span class="highlight-meta highlight-function-call highlight-shell"><span class="highlight-variable highlight-function highlight-shell">git</span></span><span class="highlight-meta highlight-function-call highlight-arguments highlight-shell"> clone git@github.com:accelerated-computing-class/lab12.git</span>
</span></pre>

<!--
<h2>Section 0 (Prelab): Memory Hierarchies and Performance of Serial Applications</h2>

<p>In this exercise you will explore how memory hierarchies affect performance for serial applications.</p>

<ul>
  <li>arithmetic intensity: how to compute; divides; memory access</li>
  <li>at least 2 different system architectures. Compare and contrast.</li>
</ul>

<p>All of these aspects of serial applications are transferable to parallel applications, as we shall see later in the course.</p>

<h3>Warm-up</h3>

<p>Review the section in <a href="../assets/EijkhoutIntroToHPC2020.pdf">HPSC</a> on computing arithmetic intensity for given compute kernels.<br>
Then, as a group, compute the arithmetic intensities of the following kernels in units of FLOPs/byte, assuming 8 bytes per float.</p>

<pre><code>  Y[j] += Y[j] + A[j][i] * B[i]
</code></pre>

<pre><code>  s += A[i] * A[i]
</code></pre>

<pre><code>  s += A[i] * B[i]
</code></pre>

<pre><code>  Y[i] = A[i] + C*B[i]
</code></pre>

<p>Included a table in your project report listing the arithmetic intensities for these kernels.</p>

<h3>Part 1: Matrix-matrix Multiplication</h3>

<p>In this first part of the project, you will test the performance of the basic matrix-matrix multiplication operation as a function of matrix size on multiple compute platforms. Complete the following using at least two different compute architectures (e.g., your laptop and HPCC, or two different node architectures on HPCC).</p>

<ol>
  <li>With your group, write a program that multiplies two matrices together (see: <a href="http://mathworld.wolfram.com/MatrixMultiplication.html">Matrix Multiplication</a>). Use GitHub to manage the code development and version history.</li>

  <li>For a given matrix size <em>N</em>, what is the total number of floating point operations performed by this operator?</li>

  <li>Using the supplied C routine <code>get_walltime.c</code>, or any other accurate means of measuring time, compute the performance in Mflop/s of the matrix-matrix multiply for <em>N</em>=100. Be sure to perform enough repeat calculations of the timing to overcome any statistical noise in the measurement.</li>

  <li>For the system you are running on, determine the clock speed of the processor and the cache size/layout. Use this information to estimate the theoretical peak performance of the system, assuming that the processor is capable of one flop per clock cycle (generally NOT true on modern architectures). How does the performance you measured in (3) compare to the theoretical peak performance of your system?</li>

  <li>Now repeat the performance measurement for a range of matrix size <code>N</code> from 1 to 10,000,000. Make a plot of the resulting measured Gflop/s vs. <code>N</code>. On this plot place a horizontal line representing the theoretical peak performance based upon your system's clock speed.</li>

  <li>How does the measured performance for multiple <em>N</em>'s compare to peak? Are there any "features" in your plot? Explain them in the context of the hardware architecture of your system. Include in your write-up a description of your system's architecture (processor, cache, etc.).</li>
</ol>

<p>To your project git repo, commit your code for performing the matrix-matrix multiply performance measurements, the plots of your results, and a brief write-up (in plain text or markdown) addressing the above questions and discussing your results. Pay particular attention to the comparison between different architectures and give explanations for them.</p>

<h3>Part 2: The Roofline Model</h3>

<p>In this part, you will explore the roofline model for analyzing the interplay between arithmetic intensity and memory bandwidth for architectures with complex memory hierarchies. Complete the following exercises on the <em>SAME</em> compute architectures that you used in Part 1 above.</p>

<ol>
  <li>Reference the materials on the Roofline Performance model at <a href="https://crd.lbl.gov/divisions/amcr/computer-science-amcr/par/research/roofline/">Roofline Model</a>. In particular, look through <a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2008/EECS-2008-134.pdf">"Roofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures"</a> and the slides at <a href="https://crd.lbl.gov/assets/pubs_presos/parlab08-roofline-talk.pdf">PARLAB Talk</a>.</li>

  <li>Clone the CS Roofline Toolkit: <code>git clone https://bitbucket.org/berkeleylab/cs-roofline-toolkit.git</code>. Modify one of the config files in <code>Empirical_Roofline_Tool-1.1.0/Config</code> as necessary for the machine you are using.</li>

  <li>Run the ERT in serial mode on your local machine. Report the peak performances and bandwidths (for all caches levels as well as DRAM). Where is the "ridge point" of the roofline for the various cases?</li>

  <li>Consider the four FP kernels in "Roofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures" (see their Table 2). Assuming the high end of operational (i.e., "arithmetic") intensity, how would these kernels perform on the platforms you are testing? What optimization strategy would you recommend to increase performance of these kernels?</li>

  <li>Address the same questions in (4) for the four kernels given in the Warm-up above.</li>

  <li>Compare your results for the roofline model to what you obtained for the matrix-matrix multiplication operation from Part 1. How are the rooflines of memory bandwidth related to the features in the algorithmic performance as a function of matrix size?</li>
</ol>

<p>To your project write-up, add your plots of the roofline model for the systems you tested, and responses addressing the above questions.</p>
-->

<h2>Section 1: Getting Started with MPI</h2>

<h3>Goals for This Lab</h3>

<p>In this project you will explore using basic MPI collectives on HPCC. After finishing this project, you should:</p>
  <ul>
    <li>understand how to parallelize a simple serial code with MPI,</li>
    <li>understand how to use basic MPI collectives,</li>
    <li>be able to run MPI-parallel applications on HPCC,</li>
    <li>develop your theoretical understanding of key parallel computing concepts such as:
      <ul>
        <li>functional parallelism,</li>
        <li>collective communication,</li>
        <li>parallel scaling, and</li>
        <li>parallel efficiency.</li>
      </ul>
    </li>
  </ul>

  <h3>Part 1: Warm-up Exercises</h3>
  <p>As a group, complete the following exercises from HPSC (<a href="../assets/EijkhoutIntroToHPC2020.pdf">Eijkhout Intro to HPC</a>).</p>
  <ul>
    <li>Exercise 2.18</li>
    <li>Exercise 2.19</li>
    <li>Exercise 2.21</li>
    <li>Exercise 2.22</li>
    <li>Exercise 2.23</li>
    <li>Exercise 2.27</li>
  </ul>
  <p>Include your responses to these exercises in your project write-up.</p>

  <h3>Part 2: Setup on HPCC</h3>
  <p>The following is a very quick tutorial on the basics of using HPCC for this class.</p>
  <ol>
    <li>Log in to the HPCC gateway:
      <pre><code>ssh &lt;netid&gt;@hpcc.msu.edu</code></pre>
    </li>
    <li>Then log in to the AMD-20 cluster from the gateway:
      <pre><code>module load powertools
amd20</code></pre>
    </li>
    <li>The default environment and software stack should be sufficient for this exercise, but if you run into issues compiling and running the code try:
      <pre><code>module purge
module load intel/2021a</code></pre>
    </li>
    <li>When using the HPCC for development and exercises in this class please <strong>do NOT</strong> just use the head node, <code>dev-amd20</code>. We will swamp the node and no one will get anything done. Instead, request an interactive job using the SLURM scheduler. An easy way to do this is to set up an alias command like so:
      <pre><code>alias devjob='salloc -n 4 --time 1:30:00'</code></pre>
    </li>
    <li>Run <code>devjob</code> to request 4 tasks for 90 minutes. This should be sufficient for most of the stuff we do during class, though for your projects you will at times require more resources. The above <code>module</code> and <code>alias</code> commands can be added to your <code>~/.bashrc</code> so that they are automatically executed when you log in.</li>
  </ol>

  <h3>Part 3: MPI Basics</h3>
  <ol>
    <li>Clone your Project 2 repo from GitHub on HPCC.</li>
    <li>In the project directory you will find a simple "Hello World!" source file in C++. Compile and run the code. Example:
      <pre><code>g++ hello.cpp
./a.out</code></pre>
    </li>
    <li>Now run the executable <code>a.out</code> using the MPI parallel run command and explain the output:
      <pre><code>mpiexec -n 4 ./a.out</code></pre>
    </li>
    <li>Add the calls <code>MPI_Init</code> and <code>MPI_Finalize</code> to your code. Put three different print statements in your code: one before the init, one between init and finalize, and one after the finalize. Recompile and run the executable in serial and with <code>mpiexec</code>, and explain the output.</li>
    <li>Complete Exercises 2.3, 2.4, and 2.5 in the <a href="../assets/EijkhoutParallelProgramming.pdf">Parallel Programming</a> book.</li>
  </ol>

  <h3>Part 4: Eat Some Pi</h3>
  <p>
    Pi is the ratio of a circle's circumference to its diameter. One Monte Carlo method to approximate &pi;:
    randomly generate points in a square that bounds a circle and compute the fraction inside the circle. If <code>f = nc / ns</code> (points in circle / total points), then <code>pi ≈ 4f</code>. More points => better approximation.
  </p>
  <ol>
    <li>
      Look at the C program <code>ser_pi_calc</code>. Extend this program using collective MPI routines to compute &pi; in parallel using the method described above. You may use C++ if you prefer.
    </li>
    <li>
      For the first iteration, perform the same number of "rounds" on each MPI rank. Measure the total runtime using <code>MPI_WTIME()</code>. Vary the number of ranks used from 1 to 4. How does the total runtime change?
    </li>
    <li>
      Now, divide the number of "rounds" up among the ranks using appropriate MPI routines to distribute the work. Run the program on 1 to 4 ranks. How does the runtime vary now?
    </li>
    <li>
      Change the number of "darts" and ranks. Use your MPI program to compute &pi; using total numbers of darts of <code>1E3</code>, <code>1E6</code>, and <code>1E9</code>. For each dart count, run on HPCC with processor counts of 1, 2, 4, 8, 16, 32, and 64. Keep track of the computed &pi; and runtimes. Use non-interactive jobs and modify <code>submitjob.sb</code> as needed.
    </li>
    <li>
      For each processor count, plot the resulting errors in your computed values of &pi; (compared to the true value) as functions of the number of darts. Use log-log scaling. What is the rate of convergence? Does it make sense? Does error or convergence rate vary with processor count? Should it?
    </li>
    <li>
      For each dart count, plot runtime versus processor count (strong scaling study). Also plot the "ideal" scaling line. Calculate parallel scaling efficiency for each dart count. Does parallel performance vary with dart count? Explain.
    </li>
    <li>
      Going further: try running on different node types on HPCC with varied core counts. Ensure runs utilize multiple nodes so the network is used. Do you see a change in communication cost when the job runs on more than one node?
    </li>
  </ol>

  <h3>What to turn-in</h3>
  <p>
    To your git project repo, commit your final working code for the above exercises and a concise write-up including:
  </p>
  <ul>
    <li>responses to the warm-up exercises,</li>
    <li>performance and accuracy data for your calculations of &pi;,</li>
    <li>the plots made in Part 4, and</li>
    <li>detailed responses to the questions posed concerning your results.</li>
  </ul>


<h2>Section 2: MPI Ping-Pong and Ring Shift</h2>

<h3>Background</h3>

<p>
The ping-pong problem is a benchmark often used to evaluate the performance of message passing interfaces (MPI) in parallel computing. 
In this problem, two processes exchange messages back and forth a specified number of times, with each process sending a message and 
receiving a message alternately. In the ping-pong, process <code>i</code> sends a message of size <code>m</code> to process <code>j</code>, 
then receives a message of size <code>m</code> back from <code>j</code>. The values of <code>i</code>, <code>j</code>, 
and <code>m</code> to use are given below.
</p>

<p>
The "ring shift" problem is similar to ping-pong. In the MPI ring shift, a group of processes is arranged in a ring, with each process 
holding a unique subset of a larger array of data. The goal is to shift the data elements by a specified number of positions around the ring, 
wrapping around the ends of the ring as necessary.
</p>

<h3>Part 1: Blocking Ping-Pong</h3>

<p>Your task is to implement the ping-pong problem using MPI in C or C++ and analyze the behavior and performance of your code. Specifically, you should:</p>

<ol>
  <li>
    Implement the ping-pong problem using MPI in C or C++. Use blocking <code>MPI_Send()</code> and 
    <code>MPI_Recv()</code> calls. You should define the number of iterations and the size of the message to be exchanged.
  </li>
  <li>
    Measure the time taken to complete the ping-pong exchange for different message sizes. Use <code>MPI_Wtime()</code> 
    to obtain timestamps before and after the exchange. Vary the message size from 2 bytes to 4 kilobytes in powers of 2 
    (i.e., 2, 4, 8, ..., 2048, 4096 bytes). Perform 100 iterations for each size.
  </li>
  <li>
    Record the total amount of data sent and received during the ping-pong exchange for each configuration.
  </li>
  <li>
    Repeat steps 2 and 3 but ensure that the 2 communicating processes reside on different physical nodes on HPCC.
  </li>
  <li>
    Plot the average communication time of a single exchange (send + receive) versus message size for the two cases. 
    Estimate the <em>latency</em> and <em>bandwidth</em>. Discuss whether they differ and why.
  </li>
  <li>
    Analyze and discuss your results. Explain the behavior of the resulting curves.
  </li>
</ol>

<h3>Part 2: Non-block Ping-Pong</h3>

<p>
Repeat Part 1 using non-blocking MPI communication (i.e., <code>MPI_Isend()</code> and <code>MPI_Irecv()</code>). 
Explicit synchronization (e.g., via <code>MPI_Wait()</code>) is required. Compare results to the blocking case.
</p>

<h3>Part 3: MPI Ring Shift</h3>

<ol>
  <li>
    Implement the MPI ring shift in C or C++ for arbitrary numbers of processes and message sizes. 
    Use <code>MPI_Sendrecv()</code> rather than separate send/recv calls.
  </li>
  <li>
    Vary message sizes from 2 bytes to 4 KB (powers of 2). Also vary the number of processes from 2 to <code>N</code>, 
    in powers of 2, where <code>N</code> is large enough that rank 0 and rank <code>N-1</code> reside on separate nodes.
  </li>
  <li>
    Compute bandwidth and latency. Plot bandwidth versus message size, with separate lines for each process count.
  </li>
  <li>
    Analyze and discuss your results. Explain the behavior of the resulting curves.
  </li>
</ol>

<h3>Part 4: Non-blocking MPI Ring Shift</h3>

<p>
Repeat Part 3 using non-blocking communication with <code>MPI_Isend()</code> and <code>MPI_Irecv()</code>. 
Compare with the blocking results.
</p>

<h3>What to turn in</h3>

<p>
Commit your final working code and a concise write-up to your git repo. Include all plots and detailed answers 
to the analysis questions.
</p>

<h2>Section 3: Advanced MPI topics</h2>

<p>In this project, you will gain experience with some of the slightly more advanced features of MPI for distributed-memory parallelism.</p>

<h3>Part 1: Latency hiding in three-point average</h3>

<p>Look at the example C++ code in the project repo. This code implements a blocking ghost zone exchange for a 1D vector assuming periodic boundary conditions. The operation performed in parallel on the vector is a simple three-point rolling averaging.</p>

<ol>
  <li>Come up with a design plan for how to adapt this code to implement "latency hiding." By this we mean to overlap communication and calculation by posting non-blocking communication calls, then instead of waiting for those calls to complete, going ahead and perform all calculations that do not depend on the data being communicated.</li>
  <li>Now, go ahead with implementing a latency-hiding version of the three point averaging function. Check that your code works and gives the correct result on 1, 2, and 4 processors.</li>
  <li>Compare the performance of the non-blocking version of the code to the blocking version of the code for multiple numbers of ranks keeping the array size per-rank fixed.</li>
</ol>

<h3>Part 2: Latency hiding with one-sided MPI</h3>

<ol>
  <li>Watch the <a href="https://youtu.be/CW2EDMRyEH8">lecture by Bill Gropp on One-sided MPI</a>.</li>
  <li>Modify your three-point averaging code to use one-sided MPI communication to achieve the halo exchanges.</li>
  <li>Implement overlapping communication and calculation then be sure to correctly handle computing the "boundary" values that depend on halo data.</li>
  <li>Verify that you get sensible results for arbitrary number of processors and global array sizes.</li>
  <li>Compare the relative performance of the non-blocking and one-sided versions of your three-point averaging code for various numbers of ranks keeping the array size per-rank fixed.</li>
</ol>

<h3>Part 3: Custom data types in MPI</h3>

<p>This exercise based on <a href="https://tech.io/playgrounds/349/introduction-to-mpi/custom-types">https://tech.io/playgrounds/349/introduction-to-mpi/custom-types.</a></p>

<p><strong>Note:</strong> the example C++ code here uses C++11 features, 
  so make sure you pass any necessary flags to your compiler. 
  This may be an issue on Macs using <code>clang</code>. 
  In that case, pass the <code>-std=c++11</code> flag to <code>clang</code> (or <code>mpic++</code>, as it were) and you should be good to go.</p>

<p>As you might have noticed, all datatypes in MPI communications are atomic types: 
  an element corresponds to one singular value. 
  Moreover, every communication forces you to use a contiguous buffer with the same datatype. 
  Sometimes, it might be desirable to give additional information and meaning to the communications by creating higher-level structures. 
  MPI allows us to do that in the form of <strong>derived</strong> or custom datatypes. 
  To make our point, let's take a simple example:</p>

<p>Let's consider a system with <code>N</code> processes where all processes are charged with generating data while process 0 centralizes and stores the data. 
  The data generated by the processes corresponds to this struct :</p>

<pre><code>struct CustomData {
  int n_values;
  double dbl_values[10];
};
</code></pre>

<p>Every process generates <code>M</code> of these custom structures, and then send them to process 0. 
  What we want here is a simple gather on process 0 of all the values, 
  but we are limited at the moment with MPI and cannot do that in a simple way. 
  If we wanted to send this kind of data structure with the knowledge we currently have, 
  we would do it in a fashion similar to that shown in <code>types_example.cpp</code>.</p>

<p>As you can see from this very naive version, everything looks a lot more complicated than it should be. 
  First we have to separate the values from every process into two tables, one for integer values, one for double values. 
  Also note how the indexing part starts to become confusing with linear indexing on the double table. 
  Then we have to gather everything in two passes and finally unpack everything in the final structure.</p>

<p>This problem could be solved in a simpler way using derived datatypes. 
  A datatype can be defined easily by specifying a sequence of couples. 
  Each couple represent a <strong>block</strong> : <code>(type, displacement)</code>. 
  The type is one of the usual types used in MPI, while the displacement indicates the offset in bytes where this data block starts in memory. For instance, if we wanted to use a structure like this :</p>

<pre><code>struct DataType {
  int int_val;
  char char_val;
  float float_val;
};
</code></pre>

<p>We could describe this, as : <code>[(int, 0), (char, 4), (float, 5)]</code>. 
  As for the example above, well the description is a bit more complicated since we have 10 double each time, 
  but the idea is the same. Now, there are multiple ways of creating datatypes in MPI. 
  For instance, there is a dedicated way to repeat the same datatype multiple times. 
  There is also a more complex way of creating datatypes by generating lists such as the one showed above. 
  We are going to see the simpler version here and the complex in the following exercise.</p>

<h3>Vectors</h3>

<p>Of course the simplest form of custom datatype is the simple repetition of the same type of data. 
  For instance, if we were handling points in a 3D reference frame, 
  then we would like to manipulate a <code>Point</code> structure with three doubles in it. 
  We can achieve this very simply using the <code>MPI_Type_contiguous</code> function. Its prototype is :</p>

<pre><code>int MPI_Type_contiguous(int count, MPI_Datatype old_type, MPI_Datatype *new_type);
</code></pre>

<p>So if we want to create a vector datatype, we can easily do :</p>

<pre><code>MPI_Datatype dt_point;
MPI_Type_contiguous(3, MPI_DOUBLE, &dt_point);
</code></pre>

<p>We are not entirely done here, we need to <strong>commit</strong> the datatype. 
  The commit operation allows MPI to generate a formal description of the buffers you will be sending and receiving. 
  This is a mandatory operation. If you don't commit but still use your new datatype in communications, 
  you are most likely to end up with invalid datatype errors. 
  You can commit by simply calling <code>MPI_Type_commit</code>.</p>

<pre><code>MPI_Type_commit(&dt_point);
</code></pre>

<p>Then we can freely use this in communications, see <code>vector_example.cpp</code></p>

<p>Let's now move on to an exercise on custom datatypes.</p>

<h3>Custom types - exercise</h3>

<p>Above we have saw how to create very basic contiguous datatypes. 
  This way of creating datatypes does not help us when we want to create datatypes that mix different basic types. 
  For instance, in the previous example, we have seen a custom structure used to store the data :</p>

<pre><code>struct CustomData {
  int n_values;
  double dbl_values[10];
};
</code></pre>

<p>To represent this using the type/displacement formalism, our datatype would look something like :</p>

<pre><code>[(int, 0), (double, 4), (double, 12), (double, 20), (double, 28), (double, 36), 
(double, 44), (double, 52), (double, 60), (double, 68), (double, 76)]
</code></pre>

<p>To simplify everything, we can convert everyone of these couples as a triplet : 
  <code>(type, start, number of elements)</code>. Thus our list simplifies to :</p>

<pre><code>[(int, 0, 1), (double, 4, 10)]
</code></pre>

<p>MPI provides us with a special function to actually convert such a list in a datatype :</p>

<pre><code>int MPI_Type_create_struct(int count, const int block_length[], 
                           const MPI_Aint displacement[], 
                           const MPI_Datatype types[], 
                           MPI_Datatype *new_type);
</code></pre>

<p>Let's see these arguments one by one. <code>count</code> is the number of elements in your list, 
  in our case we have two entries, so <code>count</code> will be 2.</p>

<p><code>block_length</code> is an array of integers, indicating, 
  for entry <code>i</code>, the number of contiguous elements of that type. 
  That will be the third value of our triplet : 1 in the <code>int</code> case, 10 in the <code>double</code> case.</p>

<p><code>displacement</code> is an array of <code>MPI_Aint</code>. 
  <code>MPI_Aint</code> stands for <code>Address integer</code>. 
  These are just a specific MPI type for integers. In our case, that's the second element of each triplet.</p>

<p><code>types</code> is an array of <code>MPI_Datatypes</code>. 
  This should be pretty obvious by now : 
  it's an array of all the different sub-types we are going to use in the custom type.</p>

<p>Finally, we store the resulting datatype in <code>new_type</code>.</p>

<p>Knowing this, you are ready to optimise the example code from above, specifically, 
  removing all the copies in memory and transferring all the data using only one gather communication.</p>

<h3>Displacements</h3>

<p>Now there is a catch with the displacement. 
  Computing manually the offsets can be tricky. 
  Although it tends to be less and less the case, some types have sizes that can vary on a system/OS basis, 
  so hardcoding the values might lead to trouble. 
  One way of doing things in a cleaner way is to use the <code>offsetof</code> macro from the standard library (You will have to include <code>stddef.h</code> in C or <code>cstddef</code> in C++). 
  <code>offsetof</code> takes two parameters : a struct and the name of one attribute of the struct. 
  It returns a <code>size_t</code> (implicitly castable in a <code>MPI_Aint</code>) corresponding to the displacement of this attribute.</p>

<p>For example if we had the following structure :</p>

<pre><code>struct MyStruct {
  int a;
  double b;
  char c;
  float d;
};
</code></pre>

<p>Then, we could define out displacement table as :</p>

<pre><code>MPI_Aint displacements[4] = {
  offsetof(MyStruct, a),
  offsetof(MyStruct, b),
  offsetof(MyStruct, c),
  offsetof(MyStruct, d)
};
</code></pre>

<h3>Exercise</h3>

<p>It's your turn to optimise the program we have made in the previous section. 
  Use <code>MPI_Type_create_struct</code> to define a derived datatype and commit it so the data can be gathered on process 0. Start with the code in <code>create_struct.cpp</code>. 
  Your output from the code should be identical to that in <code>create_struct.txt</code>.</p>

<h3>Part 4: MPI Subcommunicators</h3>

<p>In this exercise, you will gain some experience with creating subcommunicators in MPI. 
  Refer to section 6.4 of the Parallel Programming text for help.</p>

<p>Look at the non-functional code <code>mpi_subcommReduce.cpp</code>. 
  Complete this code so that <code>MPI_COMM_WORLD</code> is divided into a 2D array of ranks. 
  Split <code>MPI_COMM_WORLD</code> into a subcommunicator for each row <em>and</em> for each column. 
  Then, produce sums along each row and column of the process <code>MPI_COMM_WORLD</code> rank numbers. 
  Run your code for several different total number of ranks and row sizes. 
  Have rank 0 for each communicator output the result. Verify that it is correct.</p>

<h3>What to turn-in</h3>

<p>To your git project repo, commit your final working code for the above exercises and a concise write-up including all plots, and detailed responses to the questions posed concerning your results.</p>

</main>
<footer><p><a href="https://tech.msu.edu/technology/accessibility/">Accessibility</a></p></footer>
</body></html>