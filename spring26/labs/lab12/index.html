<!DOCTYPE html><html lang="en-us"><head><meta charset="utf-8"><title>Home</title><base href="/spring26/labs/lab10/"><meta content="width=device-width, initial-scale=1" name="viewport"><style>@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/spring26/assets/font/tex-gyre-heros/texgyreheros-regular.otf") format("opentype");
    font-weight: regular;
    font-style: regular;
}
@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/spring26/assets/font/tex-gyre-heros/texgyreheros-bold.otf") format("opentype");
    font-weight: bold;
    font-style: regular;
}
@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/spring26/assets/font/tex-gyre-heros/texgyreheros-italic.otf") format("opentype");
    font-weight: regular;
    font-style: italic;
}
@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/spring26/assets/font/tex-gyre-heros/texgyreheros-bolditalic.otf") format("opentype");
    font-weight: bold;
    font-style: italic;
}</style><link href="/spring26/assets/main.css" rel="stylesheet">
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" rel="stylesheet">
<link href="/spring26/assets/favicon.png" rel="icon" type="image/png">
</head>

<body><header><nav>
<h1><a href="/spring26/">Home</a></h1>
<p><a href="/spring26/calendar">Calendar</a></p>
<p><a href="/spring26/labs">Labs</a></p>
<p><a href="/spring26/lectures">Lectures</a></p>
<p><a href="/spring26/syllabus">Syllabus</a></p>
<p><a href="/spring26/resources">Resources</a></p>
<p><a href="/spring26/contact">Contact</a></p>
<p><a href="/spring26/piazza">Piazza</a></p>
</nav></header>

<main>
<h1>Section 1: Getting Started with MPI</h1>

<h2>Prologue: Logistics</h2>
<h3>Due Dates</h3>
<p>For this lab, you’ll be turning in the following deliverables:</p>
<ul>
<li>
<p><strong>Checkpoint</strong>: Due Friday, February 14, 11:59 pm. 
    Submit your completed code in XXX, your responses to Questions XXX, and a discussion of your progress on Part 1.</p>
</li>
<li>
<p><strong>Final Submission</strong>: Due Friday, November 21, 11:59 pm. 
    Submit your completed code for all parts of the lab, 
    as well as a PDF writeup containing your answers for Question XXX of the final writeup.</p>
</li>
</ul>
<h3>Starter Code</h3>
<p>You can get the starter code for this lab by cloning the <a href="https://github.com/accelerated-computing-class/lab12">lab
repository</a>:</p>
<pre><span class="highlight-source highlight-shell highlight-bash"><span class="highlight-meta highlight-function-call highlight-shell"><span class="highlight-variable highlight-function highlight-shell">git</span></span><span class="highlight-meta highlight-function-call highlight-arguments highlight-shell"> clone git@github.com:accelerated-computing-class/lab12.git</span>
</span></pre>

<h3>Goals for This Lab</h3>

<p>In this project you will explore using basic MPI collectives on HPCC. After finishing this project, you should:</p>
  <ul>
    <li>understand how to parallelize a simple serial code with MPI,</li>
    <li>understand how to use basic MPI collectives,</li>
    <li>be able to run MPI-parallel applications on HPCC,</li>
    <li>develop your theoretical understanding of key parallel computing concepts such as:
      <ul>
        <li>functional parallelism,</li>
        <li>collective communication,</li>
        <li>parallel scaling, and</li>
        <li>parallel efficiency.</li>
      </ul>
    </li>
  </ul>

  <h2>Part 1: Warm-up Exercises</h2>
  <p>As a group, complete the following exercises from HPSC (<a href="../assets/EijkhoutIntroToHPC2020.pdf">Eijkhout Intro to HPC</a>).</p>
  <ul>
    <li>Exercise 2.18</li>
    <li>Exercise 2.19</li>
    <li>Exercise 2.21</li>
    <li>Exercise 2.22</li>
    <li>Exercise 2.23</li>
    <li>Exercise 2.27</li>
  </ul>
  <p>Include your responses to these exercises in your project write-up.</p>

  <h2>Part 2: Setup on HPCC</h2>
  <p>The following is a very quick tutorial on the basics of using HPCC for this class.</p>
  <ol>
    <li>Log in to the HPCC gateway:
      <pre><code>ssh &lt;netid&gt;@hpcc.msu.edu</code></pre>
    </li>
    <li>Then log in to the AMD-20 cluster from the gateway:
      <pre><code>module load powertools
amd20</code></pre>
    </li>
    <li>The default environment and software stack should be sufficient for this exercise, but if you run into issues compiling and running the code try:
      <pre><code>module purge
module load intel/2021a</code></pre>
    </li>
    <li>When using the HPCC for development and exercises in this class please <strong>do NOT</strong> just use the head node, <code>dev-amd20</code>. We will swamp the node and no one will get anything done. Instead, request an interactive job using the SLURM scheduler. An easy way to do this is to set up an alias command like so:
      <pre><code>alias devjob='salloc -n 4 --time 1:30:00'</code></pre>
    </li>
    <li>Run <code>devjob</code> to request 4 tasks for 90 minutes. This should be sufficient for most of the stuff we do during class, though for your projects you will at times require more resources. The above <code>module</code> and <code>alias</code> commands can be added to your <code>~/.bashrc</code> so that they are automatically executed when you log in.</li>
  </ol>

  <h2>Part 3: MPI Basics</h2>
  <ol>
    <li>Clone your Project 2 repo from GitHub on HPCC.</li>
    <li>In the project directory you will find a simple "Hello World!" source file in C++. Compile and run the code. Example:
      <pre><code>g++ hello.cpp
./a.out</code></pre>
    </li>
    <li>Now run the executable <code>a.out</code> using the MPI parallel run command and explain the output:
      <pre><code>mpiexec -n 4 ./a.out</code></pre>
    </li>
    <li>Add the calls <code>MPI_Init</code> and <code>MPI_Finalize</code> to your code. Put three different print statements in your code: one before the init, one between init and finalize, and one after the finalize. Recompile and run the executable in serial and with <code>mpiexec</code>, and explain the output.</li>
    <li>Complete Exercises 2.3, 2.4, and 2.5 in the <a href="../assets/EijkhoutParallelProgramming.pdf">Parallel Programming</a> book.</li>
  </ol>

  <h2>Part 4: Eat Some Pi</h2>
  <p>
    Pi is the ratio of a circle's circumference to its diameter. One Monte Carlo method to approximate &pi;:
    randomly generate points in a square that bounds a circle and compute the fraction inside the circle. If <code>f = nc / ns</code> (points in circle / total points), then <code>pi ≈ 4f</code>. More points => better approximation.
  </p>
  <ol>
    <li>
      Look at the C program <code>ser_pi_calc</code>. Extend this program using collective MPI routines to compute &pi; in parallel using the method described above. You may use C++ if you prefer.
    </li>
    <li>
      For the first iteration, perform the same number of "rounds" on each MPI rank. Measure the total runtime using <code>MPI_WTIME()</code>. Vary the number of ranks used from 1 to 4. How does the total runtime change?
    </li>
    <li>
      Now, divide the number of "rounds" up among the ranks using appropriate MPI routines to distribute the work. Run the program on 1 to 4 ranks. How does the runtime vary now?
    </li>
    <li>
      Change the number of "darts" and ranks. Use your MPI program to compute &pi; using total numbers of darts of <code>1E3</code>, <code>1E6</code>, and <code>1E9</code>. For each dart count, run on HPCC with processor counts of 1, 2, 4, 8, 16, 32, and 64. Keep track of the computed &pi; and runtimes. Use non-interactive jobs and modify <code>submitjob.sb</code> as needed.
    </li>
    <li>
      For each processor count, plot the resulting errors in your computed values of &pi; (compared to the true value) as functions of the number of darts. Use log-log scaling. What is the rate of convergence? Does it make sense? Does error or convergence rate vary with processor count? Should it?
    </li>
    <li>
      For each dart count, plot runtime versus processor count (strong scaling study). Also plot the "ideal" scaling line. Calculate parallel scaling efficiency for each dart count. Does parallel performance vary with dart count? Explain.
    </li>
    <li>
      Going further: try running on different node types on HPCC with varied core counts. Ensure runs utilize multiple nodes so the network is used. Do you see a change in communication cost when the job runs on more than one node?
    </li>
  </ol>

  <h2>What to turn-in</h2>
  <p>
    To your git project repo, commit your final working code for the above exercises and a concise write-up including:
  </p>
  <ul>
    <li>responses to the warm-up exercises,</li>
    <li>performance and accuracy data for your calculations of &pi;,</li>
    <li>the plots made in Part 4, and</li>
    <li>detailed responses to the questions posed concerning your results.</li>
  </ul>


<h1>Section 2: MPI Ping-Pong and Ring Shift</h1>

<h2>Background</h2>

<p>
The ping-pong problem is a benchmark often used to evaluate the performance of message passing interfaces (MPI) in parallel computing. 
In this problem, two processes exchange messages back and forth a specified number of times, with each process sending a message and 
receiving a message alternately. In the ping-pong, process <code>i</code> sends a message of size <code>m</code> to process <code>j</code>, 
then receives a message of size <code>m</code> back from <code>j</code>. The values of <code>i</code>, <code>j</code>, 
and <code>m</code> to use are given below.
</p>

<p>
The "ring shift" problem is similar to ping-pong. In the MPI ring shift, a group of processes is arranged in a ring, with each process 
holding a unique subset of a larger array of data. The goal is to shift the data elements by a specified number of positions around the ring, 
wrapping around the ends of the ring as necessary.
</p>

<h2>Part 1: Blocking Ping-Pong</h2>

<p>Your task is to implement the ping-pong problem using MPI in C or C++ and analyze the behavior and performance of your code. Specifically, you should:</p>

<ol>
  <li>
    Implement the ping-pong problem using MPI in C or C++. Use blocking <code>MPI_Send()</code> and 
    <code>MPI_Recv()</code> calls. You should define the number of iterations and the size of the message to be exchanged.
  </li>
  <li>
    Measure the time taken to complete the ping-pong exchange for different message sizes. Use <code>MPI_Wtime()</code> 
    to obtain timestamps before and after the exchange. Vary the message size from 2 bytes to 4 kilobytes in powers of 2 
    (i.e., 2, 4, 8, ..., 2048, 4096 bytes). Perform 100 iterations for each size.
  </li>
  <li>
    Record the total amount of data sent and received during the ping-pong exchange for each configuration.
  </li>
  <li>
    Repeat steps 2 and 3 but ensure that the 2 communicating processes reside on different physical nodes on HPCC.
  </li>
  <li>
    Plot the average communication time of a single exchange (send + receive) versus message size for the two cases. 
    Estimate the <em>latency</em> and <em>bandwidth</em>. Discuss whether they differ and why.
  </li>
  <li>
    Analyze and discuss your results. Explain the behavior of the resulting curves.
  </li>
</ol>

<h2>Part 2: Non-block Ping-Pong</h2>

<p>
Repeat Part 1 using non-blocking MPI communication (i.e., <code>MPI_Isend()</code> and <code>MPI_Irecv()</code>). 
Explicit synchronization (e.g., via <code>MPI_Wait()</code>) is required. Compare results to the blocking case.
</p>

<h2>Part 3: MPI Ring Shift</h2>

<ol>
  <li>
    Implement the MPI ring shift in C or C++ for arbitrary numbers of processes and message sizes. 
    Use <code>MPI_Sendrecv()</code> rather than separate send/recv calls.
  </li>
  <li>
    Vary message sizes from 2 bytes to 4 KB (powers of 2). Also vary the number of processes from 2 to <code>N</code>, 
    in powers of 2, where <code>N</code> is large enough that rank 0 and rank <code>N-1</code> reside on separate nodes.
  </li>
  <li>
    Compute bandwidth and latency. Plot bandwidth versus message size, with separate lines for each process count.
  </li>
  <li>
    Analyze and discuss your results. Explain the behavior of the resulting curves.
  </li>
</ol>

<h2>Part 4: Non-blocking MPI Ring Shift</h2>

<p>
Repeat Part 3 using non-blocking communication with <code>MPI_Isend()</code> and <code>MPI_Irecv()</code>. 
Compare with the blocking results.
</p>

<h2>What to turn in</h2>

<p>
Commit your final working code and a concise write-up to your git repo. Include all plots and detailed answers 
to the analysis questions.
</p>

</main>
</body></html>