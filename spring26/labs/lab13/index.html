<!DOCTYPE html><html lang="en-us"><head><meta charset="utf-8"><title>Home</title><base href="/spring26/labs/lab10/"><meta content="width=device-width, initial-scale=1" name="viewport"><style>@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/spring26/assets/font/tex-gyre-heros/texgyreheros-regular.otf") format("opentype");
    font-weight: regular;
    font-style: regular;
}
@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/spring26/assets/font/tex-gyre-heros/texgyreheros-bold.otf") format("opentype");
    font-weight: bold;
    font-style: regular;
}
@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/spring26/assets/font/tex-gyre-heros/texgyreheros-italic.otf") format("opentype");
    font-weight: regular;
    font-style: italic;
}
@font-face {
    font-family: "Tex Gyre Heros";
    src: url("/spring26/assets/font/tex-gyre-heros/texgyreheros-bolditalic.otf") format("opentype");
    font-weight: bold;
    font-style: italic;
}</style><link href="/spring26/assets/main.css" rel="stylesheet">
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" rel="stylesheet">
<link href="/spring26/assets/favicon.png" rel="icon" type="image/png">
</head>

<body>
<header><nav>
<h1><a href="/spring26/">Home</a></h1>
<p><a href="/spring26/calendar">Calendar</a></p>
<p><a href="/spring26/labs">Labs</a></p>
<p><a href="/spring26/lectures">Lectures</a></p>
<p><a href="/spring26/syllabus">Syllabus</a></p>
<p><a href="/spring26/resources">Resources</a></p>
<p><a href="/spring26/contact">Contact</a></p>
<p><a href="/spring26/piazza">Piazza</a></p>
</nav></header>

<main>
<h1>Lab 13: NanoGPT</h1>
<h2>Prologue: Logistics</h2>
<h3>Due Dates</h3>
<p>For this lab, you’ll be turning in the following deliverables:</p>
<ul>
<li>
<p><strong>Checkpoint</strong>: Due Friday, February 14, 11:59 pm. 
    Submit your completed code in XXX, your responses to Questions XXX, and a discussion of your progress on Part 1.</p>
</li>
<li>
<p><strong>Final Submission</strong>: Due Friday, November 21, 11:59 pm. 
    Submit your completed code for all parts of the lab, 
    as well as a PDF writeup containing your answers for Question XXX of the final writeup.</p>
</li>
</ul>

<h3>Starter Code</h3>
<p>You can get the starter code for this lab by cloning the <a href="https://github.com/accelerated-computing-class/lab13">lab
repository</a>:</p>
<pre><span class="highlight-source highlight-shell highlight-bash"><span class="highlight-meta highlight-function-call highlight-shell"><span class="highlight-variable highlight-function highlight-shell">git</span></span><span class="highlight-meta highlight-function-call highlight-arguments highlight-shell"> clone git@github.com:accelerated-computing-class/lab13.git</span>
</span></pre>

<h3>Goals for This Lab</h3>

<p>
In this assignment, you will implement and optimize the key components of a transformer-based deep neural network that synthesizes Shakespeare text. 
While the DNN you will work with is a fairly small model, the basic components are the same as those in large-language models (LLMs) such as ChatGPT.
Specifically, you will implement the attention layer in C++, focusing on optimizations that improve arithmetic intensity, reduce memory footprint, 
and leverage multi-core and SIMD parallelism on the CPU. Your implementation will be used inside a complete 
<a href="https://github.com/karpathy/nanoGPT">NanoGPT</a> model, which you will run to generate Shakespeare-like text.
</p>

<p>This assignment will:</p>
<ul>
  <li>Give you experience with low-level DNN layer implementation—i.e., the “guts” of vendor libraries like NVIDIA CuDNN or Intel OneAPI.</li>
  <li>Show the value of locality-preserving optimizations such as loop blocking and loop fusion.</li>
</ul>

<h2>Environment Setup</h2>

<p>
We will provide SSH access to a cluster of shared machines for testing your code. Instructions will be posted on Ed.
</p>

<p>Clone the repository:</p>

<pre><code>git clone https://github.com/stanford-cs149/cs149gpt.git
</code></pre>

<p>Run inference using a pre-trained model:</p>

<pre><code>python3 gpt149.py part0 --inference -m shakes128
</code></pre>

<p>On the first run, Torch will JIT-compile your C++ code. You may see:<br>
<em>Compiling code into a PyTorch module…</em>
</p>

<p>Example output:</p>

<pre>
Running inference using dnn model shakes128
number of parameters: 0.80M
Loading meta from data/shakespeare_char/meta.pkl...

BOTtaps along my lord.

DUKE OF AUMERLE:
The this is needs! Camillo, put I will make be strong.
…
</pre>

<p>
Larger models such as <code>shakes256</code>, <code>shakes1024</code>, and <code>shakes2048</code> will run slower.
</p>

<h3>My Compilation Hangs</h3>

<p>If compilation hangs (Torch lock issue), run:</p>

<pre><code>rm ~/.cache/torch_extensions/py310_cpu/custom_module/lock
</code></pre>

<h2>An Attention Module</h2>

<p>
NanoGPT is a sequence-to-sequence transformer model. The input is a sequence of word embeddings; the output is likely next tokens. The model you build implements the attention mechanism.
</p>

<p>Attention takes 3 matrices (each size <code>N × d</code>):</p>
<ul>
  <li><strong>Q</strong> (queries)</li>
  <li><strong>K</strong> (keys)</li>
  <li><strong>V</strong> (values)</li>
</ul>

<p><strong>Important:</strong> In practice these are stored as **4D tensors** due to batching and multiple attention heads, but you'll only care about two dimensions (N × d).</p>

<h3>Step 1: Compute the similarity matrix</h3>

<pre><code>S = Q Kᵀ
</code></pre>

<h3>Step 2: Softmax each row</h3>

<p>Given a row vector <code>x</code>:</p>

<pre><code>softmax(x) = exp(x) / sum(exp(x))
</code></pre>

<p>(This version does not subtract max(x) as in the numerically safe version.)</p>

<h3>Step 3: Compute output</h3>

<pre><code>O = P V
</code></pre>

<p>Thus attention = matmul → softmax → matmul.</p>

<h2>Warm-Up: Accessing Tensors (3 Points)</h2>

<p>PyTorch tensors are multi-dimensional arrays. For Parts 1–4, tensors are converted into flat C++ vectors for you via <code>formatTensor</code>.</p>

<h3>Step 1: Understand 2D accessor</h3>

<p>Formula for accessing (i, j) in 2D:</p>

<pre><code>index = i * num_cols + j
</code></pre>

<h3>Step 2: Implement 4D accessor</h3>

<p>
Extend the idea to implement <code>fourDimRead</code> and <code>fourDimWrite</code> in <code>module.cpp</code>.
</p>

<h3>Test:</h3>

<pre><code>python3 gpt149.py 4Daccess
</code></pre>

<h3>What to Submit</h3>
<ul>
  <li>Implement <code>fourDimRead</code> and <code>fourDimWrite</code>.</li>
  <li>Write-up: Explain how a 4D tensor is laid out in memory and why this convention is efficient.</li>
</ul>

<h2>Part 1: Naive Attention (10 Points)</h2>

<p>Implement <code>myNaiveAttention</code> using serial loops.</p>

<ol>
  <li>For each batch</li>
  <li>For each head</li>
  <ol type="a">
    <li>Compute <code>QKᵀ</code> (size N × N)</li>
    <li>Softmax each row</li>
    <li>Compute <code>O = PV</code></li>
  </ol>
</ol>

<h3>Tests</h3>

<pre><code>python3 gpt149.py part1
</code></pre>

<p>Use <code>-N &lt;val&gt;</code> to test different N values.</p>

<h3>What to Submit</h3>
<ul>
  <li>Implement <code>myNaiveAttention</code> in <code>module.cpp</code>.</li>
</ul>

<h2>Part 2: Blocked Matmul + Unfused Softmax (20 Points)</h2>

<p>
Improve cache locality by performing blocked matrix multiplication for both <code>QKᵀ</code> and <code>PV</code>.
Choose a tile size; handle edge tiles with <code>min(tile, N − tile_start)</code>.
</p>

<h3>Test</h3>

<pre><code>python3 gpt149.py part2
</code></pre>

<h3>What to Submit</h3>

<ul>
  <li>Implement <code>myUnfusedAttentionBlocked</code>.</li>
  <li>Write-up:
    <ul>
      <li>Tile size experiments for N=1024 (times + conclusion).</li>
      <li>Ratio of DRAM accesses in Part 2 vs Part 1.</li>
    </ul>
  </li>
</ul>

<h2>Part 3: Fused Attention (25 Points)</h2>

<p>
Fuse the computation so you never materialize the NxN matrix. Instead:
</p>

<ol>
  <li>Compute one row of <code>QKᵀ</code></li>
  <li>Softmax it</li>
  <li>Immediately compute one row of <code>O = PV</code></li>
</ol>

<p>This reduces memory from O(N²) to O(N).</p>

<h3>Parallelization</h3>

<p>Use OpenMP:</p>

<pre><code>#pragma omp parallel for collapse(n)
</code></pre>

<p>
You may parallelize across batches, heads, and rows. Use the provided per-thread temporary buffer.
</p>

<h3>Test</h3>

<pre><code>python3 gpt149.py part3
</code></pre>

<h3>What to Submit</h3>
<ul>
  <li>Implement <code>myFusedAttention</code>.</li>
</ul>

</main>
</body></html>